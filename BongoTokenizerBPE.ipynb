{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_wX4POkNlPi",
        "outputId": "31699c58-c398-4dec-c855-0bf4a04e2988"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd drive/MyDrive/Tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgrPdEnUORAk",
        "outputId": "5978f5b3-6ee7-4c09-e16e-e7d3874f7bee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Tokenizer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUqEKbLjOhfd",
        "outputId": "3e59dc9f-4f27-41db-fc4d-23b0e3640968"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "\n",
        "#Since Bangla sentences are whitespace seperated between words\n",
        "tokenizer.pre_tokenizer = Whitespace()\n"
      ],
      "metadata": {
        "id": "7TlLrGKOOpn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#taining with combined file and save it to json file\n",
        "files = [f\"wiki_novels_combined.txt\" for split in [\"test\", \"train\", \"valid\"]]\n",
        "tokenizer.train(files, trainer)\n",
        "tokenizer.save(\"bpe-tokenizer.json\")"
      ],
      "metadata": {
        "id": "mav42t8hUM1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.get_vocab_size())"
      ],
      "metadata": {
        "id": "qTyqi3I8n9Qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load tokenize json and tokenize\n",
        "tokenizer = Tokenizer.from_file(\"bpe-tokenizer.json\")\n"
      ],
      "metadata": {
        "id": "XdQLudF1kv68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = tokenizer.encode_batch([\"সোহাগী হাইস্কুলের হেডমাষ্টার সাহেব দারুণ অবাক হলেন|\",\n",
        "                                 'বগুড়ায় জাতীয় লিগে দ্বিতীয় স্তরের ম্যাচে ঢাকা মেট্রোপলিসের হয়ে সেঞ্চুরি পেয়েছেন মাহমুদউল্লাহ।',\n",
        "                                 'রাজপণ্ডিত হব মনে আশা করে | সপ্তশ্লোক ভেটিলাম রাজা গৌড়েশ্বরে ||',\n",
        "                                 'মোর মত মাইনষে যদি সরকারি ঘর না পায় তাইলে কে পাইবে?',\n",
        "                                 'বংশী বাংলাদেশের একটি ক্ষুদ্রতম উপজাতি। এরা টাঙ্গাইল জেলার \"মহানান্দপুর\" এবং \"দন্দোনিয়া\" নামে পাশাপাশি দুইটি গ্রামের বসবাস করে।',\n",
        "                                 'রংপুর জেলার আঞ্চলিক ভাষার উদাহরণঃ এ্যাকনা মাইনসের দুকনা ব্যাটা আছিল। তার ছোটোকোনা উয়ার বাপকে কইল,বা,মোর পইসা কড়ির ভাগ মোক দেও। ওই কতাতে তাঁয় উমার ঘরক সব বাটি দিল।'])\n",
        "\n",
        "\n",
        "for item in output:\n",
        "  print(item.tokens)"
      ],
      "metadata": {
        "id": "_JHdAcpqn7aJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.get_vocab_size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4N3B9TSbmXBr",
        "outputId": "35d0b0f7-1ece-4fd8-a04a-17248732ce62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30000\n"
          ]
        }
      ]
    }
  ]
}