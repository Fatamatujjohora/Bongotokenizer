{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_wX4POkNlPi",
        "outputId": "fa5698f5-9443-428c-8fd8-021208245a04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd drive/MyDrive/Tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgrPdEnUORAk",
        "outputId": "b13c6683-aa07-4edc-c33c-0d217089fcac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'drive/MyDrive/Tokenizer'\n",
            "/content/drive/MyDrive/Tokenizer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "\n",
        "#Since Bangla sentences are whitespace seperated between words\n",
        "tokenizer.pre_tokenizer = Whitespace()\n"
      ],
      "metadata": {
        "id": "7TlLrGKOOpn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#taining with combined file and save it to json file\n",
        "files = [f\"wiki_nobel_combined_v2.txt\"]\n",
        "tokenizer.train(files, trainer)\n",
        "tokenizer.save(\"bpe-tokenizer_v2.json\")"
      ],
      "metadata": {
        "id": "mav42t8hUM1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "6d7d19b0-c1b8-443d-8ef2-d3d9b3c35b89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-3063735fd2d9>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#taining with combined file and save it to json file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34mf\"wiki_nobel_combined_v2.txt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bpe-tokenizer_v2.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.get_vocab_size())"
      ],
      "metadata": {
        "id": "qTyqi3I8n9Qe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ed54c3a-81f1-4cae-8f52-b304fb569936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load tokenize json and tokenize\n",
        "from tokenizers import Tokenizer\n",
        "tokenizer = Tokenizer.from_file(\"bpe-tokenizer_v2.json\")\n"
      ],
      "metadata": {
        "id": "XdQLudF1kv68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = tokenizer.encode_batch([\"আমি বাংলায় গান গাই।\",\n",
        "                                 \"মোর সামর্থ্য থাকলে মুই প্রয়োজনে ঘরের জনতে প্রধানমন্ত্রীর কাছে গেনু হয়।\",\n",
        "                                 'মোর মত মাইনষে যদি সরকারি ঘর না পায় তাইলে কে পাইবে?',\n",
        "                                 \"দুর ব্যাটা, অন্ধকারে তুই মানুষ দেখলি কীভাবে?\",\n",
        "                                 'বরষ! নেমেছে মোর নয়নে | ',\n",
        "                                 'ঈশ্বর থাকেন ওই গ্রামে, ভদ্রপল্লীতে। এখানে তাঁহাদের খুঁজিয়া পাওয়া যাইবে না।',\n",
        "                                 'সৃষ্টির যত দোষ যত ত্রুটিই থাকুক-না কেন, মুক্তি কেবল ঐ কাঁটাপথেই',\n",
        "                                 'মনুষ্যত্বের শিক্ষাটাই চরম শিক্ষা আর সমস্তই তার অধীন।',\n",
        "                                 \"যৌবনের শেষে শুভ্র শরৎকাল ন্যায়\"])\n",
        "\n",
        "for item in output:\n",
        "  print(item.tokens)"
      ],
      "metadata": {
        "id": "_JHdAcpqn7aJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bc6c514-f059-43b9-923d-3c17656494d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['আমি', 'বাংলায়', 'গান', 'গাই', '।']\n",
            "['মোর', 'সামর্থ্য', 'থাকলে', 'মু', 'ই', 'প্রয়োজনে', 'ঘরের', 'জন', 'তে', 'প্রধানমন্ত্রীর', 'কাছে', 'গে', 'নু', 'হয়', '।']\n",
            "['মোর', 'মত', 'মাইন', 'ষে', 'যদি', 'সরকারি', 'ঘর', 'না', 'পায়', 'তাই', 'লে', 'কে', 'পাইবে', '?']\n",
            "['দুর', 'ব্যাটা', ',', 'অন্ধকারে', 'তুই', 'মানুষ', 'দেখ', 'লি', 'কীভাবে', '?']\n",
            "['বর', 'ষ', '!', 'নেমে', 'ছে', 'মোর', 'নয়নে', '|']\n",
            "['ঈশ্বর', 'থাকেন', 'ওই', 'গ্রামে', ',', 'ভদ্র', 'পল্লী', 'তে', '।', 'এখানে', 'তাঁহাদের', 'খুঁ', 'জিয়া', 'পাওয়া', 'যাইবে', 'না', '।']\n",
            "['সৃষ্টির', 'যত', 'দোষ', 'যত', 'ত্রু', 'টিই', 'থাকুক', '-', 'না', 'কেন', ',', 'মুক্তি', 'কেবল', 'ঐ', 'কাঁটা', 'পথেই']\n",
            "['মনুষ্যত্বের', 'শিক্ষা', 'টাই', 'চরম', 'শিক্ষা', 'আর', 'সমস্তই', 'তার', 'অধীন', '।']\n",
            "['যৌবনের', 'শেষে', 'শুভ্র', 'শরৎ', 'কাল', 'ন্যায়']\n"
          ]
        }
      ]
    }
  ]
}